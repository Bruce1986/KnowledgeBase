# 反向傳播（Backpropagation）完整解釋

## 🧠 概念概述

反向傳播是一種訓練神經網路時用來 **調整權重參數的演算法**。它的核心精神是：
> 「知道錯在哪裡 → 回推哪裡導致錯 → 修正那些地方。」

藉由比較預測結果與實際答案的差距（Loss），反向地傳遞這些錯誤訊息，來更新網路中的參數，使模型預測越來越準確。

---

## 🔁 運作流程

### 1. 前向傳播（Forward Pass）
- 將輸入資料傳入網路，經由各層神經元加權與非線性轉換，得到最終的預測輸出。
- 計算損失函數 Loss（表示預測錯誤的程度）。

### 2. 反向傳播（Backward Pass）
- 利用 **鏈式法則（Chain Rule）**，由輸出層往回計算每層參數對損失的影響（偏微分）。
- 根據這些偏微分值調整參數（使用梯度下降等方法）。

---

## 🔍 範例簡化說明

假設兩層網路如下：
```text
Input x → [w1] → 隱藏層 h → [w2] → Output y_hat
```

### Step 1：前向運算
```python
h = f(w1 * x)
y_hat = g(w2 * h)
loss = L(y_hat, y)  # 例如使用 MSE 或交叉熵
```

### Step 2：反向傳播
計算損失對每個參數的偏微分：
```python
∂L/∂w2 = ∂L/∂y_hat * ∂y_hat/∂w2
∂L/∂w1 = ∂L/∂y_hat * ∂y_hat/∂h * ∂h/∂w1
```

更新參數（假設學習率為 lr）：
```python
w1 = w1 - lr * ∂L/∂w1
w2 = w2 - lr * ∂L/∂w2
```

---

## 🧩 關鍵意義

- 讓神經網路能 **自動學習**。
- 使得深層網路如 CNN、RNN、Transformer 等得以訓練。
- 與 **梯度下降** 密不可分：反向傳播算出梯度，梯度下降再根據梯度更新參數。

---

## ⚠️ 常見混淆點

| 名稱 | 說明 |
|------|------|
| 反向傳播（Backpropagation） | 負責計算參數的梯度 |
| 梯度下降（Gradient Descent） | 使用這些梯度來更新參數 |
| autograd / autodiff | 現代框架中實現反向傳播的工具，如 PyTorch 的 `autograd` |

---

## 📚 延伸主題

- 鏈式法則數學推導
- 梯度消失與梯度爆炸
- 反向傳播與 ReLU 的搭配效果
- 在 PyTorch 實作反向傳播

---

> 📂 分類：`機器學習 / 深度學習基礎`  
> 🏷️ 標籤：`backpropagation` `神經網路訓練` `梯度` `反向傳播` `鏈式法則`
```
